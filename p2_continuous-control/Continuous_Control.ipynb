{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.18 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch, time\n",
    "from unityagents import UnityEnvironment\n",
    "from workspace_utils import active_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726624e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Implementaion\n",
    "\n",
    "#### Design Story\n",
    "\n",
    "I based most of my solution off of the ddpg-pendulum project in the class repository. I uesd that implementaion as a starting point and expanded from there. I had a lot of trouble with this project, my first successful agent hit the 30+ score mark at episode 4 and then carried on at 30+ average  until around episode 80 before going back down into the high twenties. This took a lot of time since my model was slow to train and very frustrating. My model design resulted in that early success but also lead to my downfall. I found that batch normalization layers were very effectoive for this project and had 2 of my fully connected layers batch normalized. This resulted in very large early rewards but very slow train rates. I thought I was on the right track with the Batch Normalization layers so I ended up tweaking almost every other parameter and used all my GPU time before realizing that the Batch Normalization layers were part of the problem. I ended up lowering the number of BN layers to 1, which improved my episode time, but reduced the rate of reward gain. In the end it seemed much more stable and was able to carry on an average of 30+ reward for over 100 episodes. This was a good lesson on using too much of a good thing, and trying to find a balance.\n",
    "\n",
    "#### Model\n",
    "\n",
    "##### Agent\n",
    "The agent model contained 3 fully connected layers, along with my final Batch normailzation layer whis is positioned after the first fullyt connected layer. The interior layers use RELU activation functions and the output layer is a tanh function.\n",
    "\n",
    "Layers\n",
    "  Input: The state size, in this case 33\n",
    "  FC1: 400 Nodes \n",
    "  BN: 400 Nodes\n",
    "  FC2: 300 Nodes \n",
    "  Output: 4 Nodes\n",
    "\n",
    "##### Critic\n",
    "The critic model contains the exact same structure as the agent, using the 3 fully connected layers and the single batch normalizer. The main differences in the forward step is that we do the action-state concatenation after the first fully connected layer and batch normalizing layer. The concat is then passed through the second fully connected layer and a RELU activation before heading to the output.\n",
    "\n",
    "Layers\n",
    "  Input: The state size, in this case 33\n",
    "  FC1: 400 Nodes \n",
    "  BN: 400 Nodes\n",
    "  FC2: 300 Nodes \n",
    "  Output: 1 Value\n",
    "\n",
    "\n",
    "#### Hyper Parameters \n",
    "\n",
    "Number of Episodes: Unknown, set to terminate after hitting 100 30+ episodes or hitting 300 total episodes  \n",
    "Max Time Steps: 1000  \n",
    "Replay Buffer Size: int(1e6)  \n",
    "Mini Batch Size: 128  \n",
    "Discount Factor: 0.99  \n",
    "Soft Update for Target Params: 1e-3  \n",
    "Actor Learning rate: 1e-3  \n",
    "Critic Learning rate: 1e-3  \n",
    "Critic Weight Decay: 0  \n",
    "Volatility Parameter: 0.2     \n",
    "Speed of Mean Reversion: 0.15  \n",
    "Noise Multiplier: 1.0  \n",
    "Noise Multiplyer Reduction Rate: 1e-6  \n",
    "\n",
    "#### Learning Algorithm\n",
    " \n",
    "For this project I implemented a Deep Deterministic Policy Gradient like we learned in this section of the course. I based this implementation on the ddpg-pendulum project shared in the repository. After initializing the capture variables, we iterate over the agents and begin training episodes. At the start of each episode, we grab the fresh environment info, observe the states, initialize the scores, and reset the agents. We then step into a nested loop which begins the traing steps for the agents. In this nested loop, the agents act on the observed states, then recieve a reward based on the action, and use that to train the models based in the results. This nested loop can be exited either by reaching the max_t parameter value or if any environment done value is flipped. Once we exit the nested loop, we accumulate the scores observed and take the mean value of the output. \n",
    "\n",
    "In addition to the original algorithm, I added a few quality of life measures to check for the provided succes state in the project. This was in place since the project takes very long to train, and I did not want to waste any more GPU time then I already had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(max_episodes=300, max_t=1000, score_d=100, print_every=1):\n",
    "    \"\"\"\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        max_episodes (int)    : maximum number of training episodes\n",
    "        max_t (int)           : maximum number of timesteps per episode\n",
    "        score_d (int)         : num episodes used to calcu score\n",
    "        print_every (int)     : interval to display results\n",
    "    \"\"\"\n",
    "    has_saved = False\n",
    "    hit_metric_move = False\n",
    "    hit_metric_curr = False\n",
    "    good_iters_move = 0\n",
    "    good_iters_curr = 0\n",
    "    \n",
    "    final_scores = []                              \n",
    "    scores_deque = deque(maxlen=score_d)  \n",
    "    moving_avgs = []     \n",
    "    \n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations                       \n",
    "        scores = np.zeros(num_agents)                           \n",
    "        agent.reset()\n",
    "        start_t = time.time()\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states, add_noise=True)       \n",
    "            env_info = env.step(actions)[brain_name]          \n",
    "            next_states = env_info.vector_observations         \n",
    "            rewards = env_info.rewards                     \n",
    "            dones = env_info.local_done                         \n",
    "\n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done, t)\n",
    "                \n",
    "            states = next_states\n",
    "            scores += rewards        \n",
    "            if np.any(dones):  break\n",
    "                \n",
    "        scores_deque.append(np.mean(scores))\n",
    "        final_scores.append(np.mean(scores))\n",
    "        moving_avgs.append(np.mean(scores_deque))\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Time: {:.2f}  Episode {}  Average Score: {:.2f}  Moving Average: {:.2f}'.format(time.time() - start_t, i_episode, scores_deque[-1], moving_avgs[-1]))\n",
    "        if final_scores[-1] >= 30 and (not has_saved or i_episode%10 == 0):\n",
    "            print('Writing File')\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            has_saved = True\n",
    "            \n",
    "        if moving_avgs[-1] >=30:\n",
    "            if hit_metric_move: good_iters_move += 1\n",
    "            else:\n",
    "                hit_metric_move = True\n",
    "                good_iters_move += 1\n",
    "            if good_iters_move >= 100: \n",
    "                torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "                torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "                return final_scores, moving_avgs\n",
    "        elif hit_metric_move:\n",
    "            good_iters_move = 0\n",
    "            hit_metric_move = False\n",
    "#             if max_episodes-i_episode < 100: return None\n",
    "            \n",
    "        if final_scores[-1] >=30:\n",
    "            if hit_metric_curr: good_iters_curr += 1\n",
    "            else:\n",
    "                hit_metric_curr = True\n",
    "                good_iters_curr += 1\n",
    "            if good_iters_curr >= 100: \n",
    "                torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "                torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "                return final_scores, moving_avgs\n",
    "        elif hit_metric_curr:\n",
    "            good_iters_curr = 0\n",
    "            hit_metric_curr = False\n",
    "#             if max_episodes-i_episode < 100: return None\n",
    "\n",
    "    return final_scores, moving_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 126.12  Episode 1  Average Score: 0.61  Moving Average: 0.61\n",
      "Time: 141.80  Episode 2  Average Score: 0.92  Moving Average: 0.77\n",
      "Time: 114.75  Episode 3  Average Score: 1.98  Moving Average: 1.17\n",
      "Time: 115.86  Episode 4  Average Score: 2.48  Moving Average: 1.50\n",
      "Time: 116.52  Episode 5  Average Score: 2.90  Moving Average: 1.78\n",
      "Time: 117.79  Episode 6  Average Score: 4.37  Moving Average: 2.21\n",
      "Time: 119.87  Episode 7  Average Score: 3.50  Moving Average: 2.40\n",
      "Time: 120.54  Episode 8  Average Score: 4.89  Moving Average: 2.71\n",
      "Time: 122.09  Episode 9  Average Score: 5.77  Moving Average: 3.05\n",
      "Time: 122.50  Episode 10  Average Score: 6.86  Moving Average: 3.43\n",
      "Time: 123.80  Episode 11  Average Score: 8.12  Moving Average: 3.85\n",
      "Time: 125.29  Episode 12  Average Score: 7.96  Moving Average: 4.20\n",
      "Time: 126.52  Episode 13  Average Score: 7.16  Moving Average: 4.43\n",
      "Time: 128.05  Episode 14  Average Score: 9.00  Moving Average: 4.75\n",
      "Time: 129.92  Episode 15  Average Score: 8.34  Moving Average: 4.99\n",
      "Time: 131.57  Episode 16  Average Score: 8.64  Moving Average: 5.22\n",
      "Time: 133.07  Episode 17  Average Score: 8.51  Moving Average: 5.41\n",
      "Time: 136.06  Episode 18  Average Score: 7.84  Moving Average: 5.55\n",
      "Time: 137.99  Episode 19  Average Score: 9.91  Moving Average: 5.78\n",
      "Time: 140.69  Episode 20  Average Score: 9.77  Moving Average: 5.98\n",
      "Time: 143.25  Episode 21  Average Score: 12.85  Moving Average: 6.30\n",
      "Time: 145.00  Episode 22  Average Score: 12.01  Moving Average: 6.56\n",
      "Time: 147.81  Episode 23  Average Score: 12.43  Moving Average: 6.82\n",
      "Time: 150.35  Episode 24  Average Score: 13.23  Moving Average: 7.09\n",
      "Time: 151.94  Episode 25  Average Score: 14.28  Moving Average: 7.37\n",
      "Time: 155.23  Episode 26  Average Score: 12.96  Moving Average: 7.59\n",
      "Time: 157.88  Episode 27  Average Score: 14.64  Moving Average: 7.85\n",
      "Time: 160.98  Episode 28  Average Score: 15.94  Moving Average: 8.14\n",
      "Time: 163.31  Episode 29  Average Score: 15.73  Moving Average: 8.40\n",
      "Time: 165.40  Episode 30  Average Score: 15.20  Moving Average: 8.63\n",
      "Time: 167.77  Episode 31  Average Score: 14.83  Moving Average: 8.83\n",
      "Time: 171.02  Episode 32  Average Score: 15.49  Moving Average: 9.04\n",
      "Time: 173.48  Episode 33  Average Score: 18.45  Moving Average: 9.32\n",
      "Time: 175.33  Episode 34  Average Score: 18.38  Moving Average: 9.59\n",
      "Time: 178.55  Episode 35  Average Score: 18.04  Moving Average: 9.83\n",
      "Time: 181.13  Episode 36  Average Score: 18.22  Moving Average: 10.06\n",
      "Time: 186.52  Episode 37  Average Score: 19.49  Moving Average: 10.32\n",
      "Time: 187.96  Episode 38  Average Score: 20.23  Moving Average: 10.58\n",
      "Time: 192.07  Episode 39  Average Score: 21.05  Moving Average: 10.85\n",
      "Time: 194.04  Episode 40  Average Score: 21.16  Moving Average: 11.10\n",
      "Time: 196.58  Episode 41  Average Score: 22.14  Moving Average: 11.37\n",
      "Time: 200.50  Episode 42  Average Score: 24.09  Moving Average: 11.68\n",
      "Time: 203.00  Episode 43  Average Score: 26.38  Moving Average: 12.02\n",
      "Time: 203.87  Episode 44  Average Score: 24.90  Moving Average: 12.31\n",
      "Time: 206.96  Episode 45  Average Score: 27.19  Moving Average: 12.64\n",
      "Time: 207.15  Episode 46  Average Score: 28.53  Moving Average: 12.99\n",
      "Time: 211.64  Episode 47  Average Score: 28.86  Moving Average: 13.32\n",
      "Time: 214.80  Episode 48  Average Score: 30.02  Moving Average: 13.67\n",
      "Writing File\n",
      "Time: 216.48  Episode 49  Average Score: 31.02  Moving Average: 14.03\n",
      "Time: 219.83  Episode 50  Average Score: 31.56  Moving Average: 14.38\n",
      "Writing File\n",
      "Time: 220.40  Episode 51  Average Score: 32.60  Moving Average: 14.73\n",
      "Time: 222.61  Episode 52  Average Score: 32.87  Moving Average: 15.08\n",
      "Time: 221.98  Episode 53  Average Score: 33.07  Moving Average: 15.42\n",
      "Time: 222.24  Episode 54  Average Score: 34.45  Moving Average: 15.77\n",
      "Time: 221.95  Episode 55  Average Score: 34.05  Moving Average: 16.11\n",
      "Time: 222.20  Episode 56  Average Score: 33.82  Moving Average: 16.42\n",
      "Time: 223.40  Episode 57  Average Score: 34.20  Moving Average: 16.73\n",
      "Time: 221.67  Episode 58  Average Score: 34.88  Moving Average: 17.05\n",
      "Time: 224.23  Episode 59  Average Score: 36.83  Moving Average: 17.38\n",
      "Time: 228.36  Episode 60  Average Score: 36.39  Moving Average: 17.70\n",
      "Writing File\n",
      "Time: 225.61  Episode 61  Average Score: 36.69  Moving Average: 18.01\n",
      "Time: 222.38  Episode 62  Average Score: 36.32  Moving Average: 18.31\n",
      "Time: 221.69  Episode 63  Average Score: 36.68  Moving Average: 18.60\n",
      "Time: 223.45  Episode 64  Average Score: 37.31  Moving Average: 18.89\n",
      "Time: 221.81  Episode 65  Average Score: 38.28  Moving Average: 19.19\n",
      "Time: 222.69  Episode 66  Average Score: 38.34  Moving Average: 19.48\n",
      "Time: 218.85  Episode 67  Average Score: 38.98  Moving Average: 19.77\n",
      "Time: 222.20  Episode 68  Average Score: 38.71  Moving Average: 20.05\n",
      "Time: 221.43  Episode 69  Average Score: 38.81  Moving Average: 20.32\n",
      "Time: 220.42  Episode 70  Average Score: 38.18  Moving Average: 20.58\n",
      "Writing File\n",
      "Time: 222.09  Episode 71  Average Score: 38.64  Moving Average: 20.83\n",
      "Time: 220.71  Episode 72  Average Score: 39.14  Moving Average: 21.08\n",
      "Time: 220.99  Episode 73  Average Score: 38.91  Moving Average: 21.33\n",
      "Time: 219.20  Episode 74  Average Score: 38.36  Moving Average: 21.56\n",
      "Time: 218.84  Episode 75  Average Score: 38.48  Moving Average: 21.78\n",
      "Time: 219.96  Episode 76  Average Score: 38.40  Moving Average: 22.00\n",
      "Time: 218.81  Episode 77  Average Score: 38.95  Moving Average: 22.22\n",
      "Time: 220.63  Episode 78  Average Score: 39.06  Moving Average: 22.44\n",
      "Time: 222.27  Episode 79  Average Score: 39.27  Moving Average: 22.65\n",
      "Time: 218.86  Episode 80  Average Score: 39.25  Moving Average: 22.86\n",
      "Writing File\n",
      "Time: 218.83  Episode 81  Average Score: 39.11  Moving Average: 23.06\n",
      "Time: 218.95  Episode 82  Average Score: 38.92  Moving Average: 23.25\n",
      "Time: 219.67  Episode 83  Average Score: 39.07  Moving Average: 23.44\n",
      "Time: 222.93  Episode 84  Average Score: 38.78  Moving Average: 23.63\n",
      "Time: 221.91  Episode 85  Average Score: 38.99  Moving Average: 23.81\n",
      "Time: 220.42  Episode 86  Average Score: 39.11  Moving Average: 23.99\n",
      "Time: 219.37  Episode 87  Average Score: 39.10  Moving Average: 24.16\n",
      "Time: 218.77  Episode 88  Average Score: 38.95  Moving Average: 24.33\n",
      "Time: 219.36  Episode 89  Average Score: 38.91  Moving Average: 24.49\n",
      "Time: 219.20  Episode 90  Average Score: 38.70  Moving Average: 24.65\n",
      "Writing File\n",
      "Time: 218.81  Episode 91  Average Score: 38.51  Moving Average: 24.80\n",
      "Time: 219.24  Episode 92  Average Score: 38.69  Moving Average: 24.95\n",
      "Time: 219.22  Episode 93  Average Score: 38.24  Moving Average: 25.10\n",
      "Time: 219.41  Episode 94  Average Score: 38.56  Moving Average: 25.24\n",
      "Time: 219.84  Episode 95  Average Score: 38.78  Moving Average: 25.38\n",
      "Time: 222.89  Episode 96  Average Score: 37.85  Moving Average: 25.51\n",
      "Time: 222.32  Episode 97  Average Score: 36.46  Moving Average: 25.62\n",
      "Time: 222.14  Episode 98  Average Score: 36.39  Moving Average: 25.73\n",
      "Time: 225.36  Episode 99  Average Score: 38.00  Moving Average: 25.86\n",
      "Time: 222.56  Episode 100  Average Score: 38.20  Moving Average: 25.98\n",
      "Writing File\n",
      "Time: 221.54  Episode 101  Average Score: 38.93  Moving Average: 26.36\n",
      "Time: 223.27  Episode 102  Average Score: 38.39  Moving Average: 26.74\n",
      "Time: 221.55  Episode 103  Average Score: 37.98  Moving Average: 27.10\n",
      "Time: 220.66  Episode 104  Average Score: 37.67  Moving Average: 27.45\n",
      "Time: 223.29  Episode 105  Average Score: 37.68  Moving Average: 27.80\n",
      "Time: 223.01  Episode 106  Average Score: 37.69  Moving Average: 28.13\n",
      "Time: 222.60  Episode 107  Average Score: 38.10  Moving Average: 28.48\n",
      "Time: 220.99  Episode 108  Average Score: 38.21  Moving Average: 28.81\n",
      "Time: 220.56  Episode 109  Average Score: 38.89  Moving Average: 29.14\n",
      "Time: 220.95  Episode 110  Average Score: 38.63  Moving Average: 29.46\n",
      "Writing File\n",
      "Time: 220.20  Episode 111  Average Score: 37.84  Moving Average: 29.76\n",
      "Time: 220.82  Episode 112  Average Score: 37.76  Moving Average: 30.06\n",
      "Time: 220.81  Episode 113  Average Score: 38.49  Moving Average: 30.37\n",
      "Time: 220.71  Episode 114  Average Score: 38.57  Moving Average: 30.66\n",
      "Time: 220.49  Episode 115  Average Score: 39.04  Moving Average: 30.97\n",
      "Time: 220.09  Episode 116  Average Score: 37.79  Moving Average: 31.26\n",
      "Time: 223.30  Episode 117  Average Score: 38.15  Moving Average: 31.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 226.22  Episode 118  Average Score: 38.19  Moving Average: 31.86\n",
      "Time: 225.61  Episode 119  Average Score: 38.52  Moving Average: 32.15\n",
      "Time: 224.37  Episode 120  Average Score: 38.79  Moving Average: 32.44\n",
      "Writing File\n",
      "Time: 221.66  Episode 121  Average Score: 38.48  Moving Average: 32.70\n",
      "Time: 221.10  Episode 122  Average Score: 37.98  Moving Average: 32.95\n",
      "Time: 223.13  Episode 123  Average Score: 37.70  Moving Average: 33.21\n",
      "Time: 225.42  Episode 124  Average Score: 37.83  Moving Average: 33.45\n",
      "Time: 225.36  Episode 125  Average Score: 37.96  Moving Average: 33.69\n",
      "Time: 222.80  Episode 126  Average Score: 38.66  Moving Average: 33.95\n",
      "Time: 221.65  Episode 127  Average Score: 39.05  Moving Average: 34.19\n",
      "Time: 222.13  Episode 128  Average Score: 37.38  Moving Average: 34.41\n",
      "Time: 223.86  Episode 129  Average Score: 38.12  Moving Average: 34.63\n",
      "Time: 224.93  Episode 130  Average Score: 38.20  Moving Average: 34.86\n",
      "Writing File\n",
      "Time: 226.67  Episode 131  Average Score: 37.89  Moving Average: 35.09\n",
      "Time: 226.46  Episode 132  Average Score: 39.02  Moving Average: 35.33\n",
      "Time: 224.91  Episode 133  Average Score: 38.96  Moving Average: 35.53\n",
      "Time: 224.25  Episode 134  Average Score: 38.78  Moving Average: 35.73\n",
      "Time: 224.36  Episode 135  Average Score: 37.97  Moving Average: 35.93\n",
      "Time: 224.38  Episode 136  Average Score: 38.73  Moving Average: 36.14\n",
      "Time: 228.91  Episode 137  Average Score: 38.66  Moving Average: 36.33\n",
      "Time: 230.05  Episode 138  Average Score: 38.59  Moving Average: 36.51\n",
      "Time: 231.20  Episode 139  Average Score: 38.65  Moving Average: 36.69\n",
      "Time: 228.97  Episode 140  Average Score: 38.05  Moving Average: 36.86\n",
      "Writing File\n",
      "Time: 228.67  Episode 141  Average Score: 36.96  Moving Average: 37.01\n",
      "Time: 225.89  Episode 142  Average Score: 37.87  Moving Average: 37.15\n",
      "Time: 231.90  Episode 143  Average Score: 38.39  Moving Average: 37.27\n",
      "Time: 227.77  Episode 144  Average Score: 38.52  Moving Average: 37.40\n",
      "Time: 228.04  Episode 145  Average Score: 38.21  Moving Average: 37.51\n",
      "Time: 230.67  Episode 146  Average Score: 37.77  Moving Average: 37.60\n",
      "Time: 229.86  Episode 147  Average Score: 38.07  Moving Average: 37.70\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ab5c24d68>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with active_session():\n",
    "    agent = Agent(state_size=state_size, action_size=action_size, random_seed=1)\n",
    "    scores, avgs = ddpg()\n",
    "    env.close()\n",
    "    \n",
    "    # plot the scores\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores, label='DDPG')\n",
    "    plt.plot(np.arange(len(scores)), avgs, c='r', label='moving avg')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Notes\n",
    "\n",
    "#### Results\n",
    "\n",
    "The agent completed the required 100 episodes at 30+ reward after a total of 147 episodes. The agent hit a clear plato around 38 reward score, and was mostly steady at the reward for the 100 iterations.\n",
    "\n",
    "#### Future Improvements\n",
    "\n",
    "I need a break before cooming back to monkey with this one. I think there is a lot of potential for improvement that I ran out of time to find. I think there would be a lot of potential in figuring out exactly why my 2 batch normalization layers were having trouble after a while, where the single layer performs very well. I also did not tweak the hyper paramters very much after finding out my batch normaization issue, so there is most likely room to improve there. \n",
    "\n",
    "The final area that would be nice to take a look at is the efficency of the learning. My solution takes a very long time to train, even on the provided work space. Well I do not think it is something that will instantly train, I do believe that I could find ways to reduce episode time well maintaining the reward rate shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
